{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"SEO_project_01\",\n",
    "\n",
    "    # # track hyperparameters and run metadata\n",
    "    # config={\n",
    "    # \"learning_rate\": 0.02,\n",
    "    # \"epochs\": 10,\n",
    "    # }\n",
    ")\n",
    "\n",
    "# # simulate training\n",
    "# epochs = 10\n",
    "# offset = random.random() / 5\n",
    "# for epoch in range(2, epochs):\n",
    "#     acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "#     loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        with open(annotation_file) as f:\n",
    "            self.coco_data = json.load(f)\n",
    "\n",
    "        self.images = self.coco_data['images']\n",
    "        self.annotations = self.coco_data['annotations']\n",
    "\n",
    "        # Mapping from image ID to its annotations\n",
    "        self.img_id_to_annotations = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_id_to_annotations:\n",
    "                self.img_id_to_annotations[img_id] = []\n",
    "            self.img_id_to_annotations[img_id].append(ann)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations for the current image\n",
    "        img_id = img_info['id']\n",
    "        annotations = self.img_id_to_annotations.get(img_id, [])\n",
    "\n",
    "        # Extract boxes and labels from annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            bbox = ann['bbox']\n",
    "            # Convert COCO bbox (x, y, w, h) to (x1, y1, x2, y2)\n",
    "            x1, y1, w, h = bbox\n",
    "            x2, y2 = x1 + w, y1 + h\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert data to tensor format\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Transforms (optional: for data augmentation)\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(torchvision.transforms.ToTensor())  # 수정된 부분\n",
    "    return torchvision.transforms.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(RetinaNetModel, self).__init__()\n",
    "        # Pretrained RetinaNet model\n",
    "        self.model = retinanet_resnet50_fpn(weights_backbone='DEFAULT', num_classes=11)\n",
    "        \n",
    "        # Modify the number of classes for the model\n",
    "        # self.model.head.classification_head.num_classes = num_classes\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        # Forward pass for validation\n",
    "        loss_dict = self.model(images, targets)\n",
    "        val_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = self.extract_preds(images)\n",
    "        targets_labels = [target['labels'] for target in targets]\n",
    "        self.val_acc(preds, targets_labels)\n",
    "\n",
    "        # Log validation loss for each batch\n",
    "        self.log('val_loss', val_loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Log validation accuracy at the end of each epoch\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def extract_preds(self, images):\n",
    "        \"\"\"Extract predictions in the correct format for accuracy calculation.\"\"\"\n",
    "        preds = self.model(images)\n",
    "        preds_labels = [pred['labels'] for pred in preds]\n",
    "        return preds_labels\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dir, train_ann, test_dir, test_ann, batch_size=4):\n",
    "        super().__init__()\n",
    "        self.train_dir = train_dir\n",
    "        self.train_ann = train_ann\n",
    "        self.test_dir = test_dir\n",
    "        self.test_ann = test_ann\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = COCODataset(self.train_dir, self.train_ann, transforms=get_transform(train=True))\n",
    "        self.test_dataset = COCODataset(self.test_dir, self.test_ann, transforms=get_transform(train=False))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths to your dataset\n",
    "    train_dir = '/data/ephemeral/level2-objectdetection-cv-15/dataset'\n",
    "    train_ann = '/data/ephemeral/level2-objectdetection-cv-15/dataset/train.json'\n",
    "    test_dir = '/data/ephemeral/level2-objectdetection-cv-15/dataset'\n",
    "    test_ann = '/data/ephemeral/level2-objectdetection-cv-15/dataset/test.json'\n",
    "\n",
    "    # Number of classes (including background as class 0)\n",
    "    # num_classes = 11  # Change this to the number of classes in your dataset\n",
    "\n",
    "    # Initialize the data module and the RetinaNet model\n",
    "    data_module = COCODataModule(train_dir, train_ann, test_dir, test_ann, batch_size=4)\n",
    "    model = RetinaNetModel()\n",
    "\n",
    "    # # Initialize trainer with updated arguments\n",
    "    trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=10)\n",
    "\n",
    "\n",
    "    # # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
