{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
    "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maforalex98\u001b[0m (\u001b[33msihari1115-chung-ang-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/level2-objectdetection-cv-15/jseo/baseline/faster_rcnn/wandb/run-20241001_203043-4wrszijo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01/runs/4wrszijo' target=\"_blank\">generous-grass-5</a></strong> to <a href='https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01' target=\"_blank\">https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01/runs/4wrszijo' target=\"_blank\">https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01/runs/4wrszijo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sihari1115-chung-ang-university/SEO_project_01/runs/4wrszijo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f79408fe950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb.init(project='SEO_project_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "      train: True일 경우 훈련 데이터, False일 경우 검증 데이터\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None, train=True, split_ratio=0.8):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.coco = COCO(annotation)\n",
    "        \n",
    "        # 이미지 IDs 가져오기\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "\n",
    "        # 데이터 분할\n",
    "        random.shuffle(self.img_ids)  # 무작위로 섞기\n",
    "        split_idx = int(len(self.img_ids) * split_ratio)\n",
    "        if train:\n",
    "            self.img_ids = self.img_ids[:split_idx]  # 훈련 데이터\n",
    "        else:\n",
    "            self.img_ids = self.img_ids[split_idx:]  # 검증 데이터\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.img_ids[index]  # 분할된 이미지 ID 사용\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxes (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        # class_id를 1~10으로 수정 \n",
    "        labels = np.array([x['category_id'] + 1 for x in anns]) \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "                                  \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([image_id]),\n",
    "            'area': areas,\n",
    "            'iscrowd': is_crowds\n",
    "        }\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_ids)  # 나눠진 데이터의 길이 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 증강 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시 적용할 데이터 증강 기법 정의\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(1024, 1024),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "# 검증 시 적용할 전처리 정의\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유틸 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 계산하며 손실 최소값에 대한 체크포인트를 차후에 만드려는 함수인 듯\n",
    "# 라이트닝에서 자동으로 체크포인트 저장해주니까 안써도 될 것 같음\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader에 전달하여 batch 생성 방법론 정의 (현재 기본 형태)\n",
    "def custom_collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(pl.LightningModule):\n",
    "    def __init__(self, train_dataset, val_dataset, batch_size=16, lr=5e-3, lr_backbone=5e-5, weight_decay=5e-4):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # model load\n",
    "        self.num_classes = 11 # 10 classes + 1 (background)\n",
    "\n",
    "        # get number of input features for the classifier\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # classifier 재정의 (위에서 계산한 in_features, num_classes)\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        self.param_dicts = {\n",
    "            \"etc\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad],\n",
    "            \"backbone\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad]\n",
    "            }\n",
    "\n",
    "\n",
    "        # hyperparameter\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # dataset\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        if targets:  # 모델이 학습 중일 때\n",
    "            return self.model(images, targets)  # 손실을 포함한 결과 반환\n",
    "        else:  # 모델이 추론 중일 때\n",
    "            return self.model(images)  # 예측 결과 반환\n",
    "\n",
    "    # 이해 굳이 안해도 되는 부분\n",
    "    # faster r-cnn이 eval 모드에서 loss를 출력하지 않아 eval 모드에서 loss_dict을 함께 output으로 출력하는 코드 가져옴\n",
    "    def eval_forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                It returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "        \"\"\"\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        original_image_sizes: List[Tuple[int, int]] = []\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            assert len(val) == 2\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        images, targets = model.transform(images, targets)\n",
    "\n",
    "        # Check for degenerate boxes\n",
    "        # TODO: Move this to a function\n",
    "        if targets is not None:\n",
    "            for target_idx, target in enumerate(targets):\n",
    "                boxes = target[\"boxes\"]\n",
    "                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "                if degenerate_boxes.any():\n",
    "                    # print the first degenerate box\n",
    "                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "                    degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "                    raise ValueError(\n",
    "                        \"All bounding boxes should have positive height and width.\"\n",
    "                        f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
    "                    )\n",
    "\n",
    "        features = model.backbone(images.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\", features)])\n",
    "        model.rpn.training=True\n",
    "        #model.roi_heads.training=True\n",
    "\n",
    "\n",
    "        #####proposals, proposal_losses = model.rpn(images, features, targets)\n",
    "        features_rpn = list(features.values())\n",
    "        objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
    "        anchors = model.rpn.anchor_generator(images, features_rpn)\n",
    "\n",
    "        num_images = len(anchors)\n",
    "        num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "        num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "        objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "        # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "        # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "        # the proposals\n",
    "        proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "        proposals = proposals.view(num_images, -1, 4)\n",
    "        proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "        proposal_losses = {}\n",
    "        assert targets is not None\n",
    "        labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
    "        regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
    "        loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
    "            objectness, pred_bbox_deltas, labels, regression_targets\n",
    "        )\n",
    "        proposal_losses = {\n",
    "            \"loss_objectness\": loss_objectness,\n",
    "            \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "        }\n",
    "\n",
    "        #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "        image_shapes = images.image_sizes\n",
    "        proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
    "        box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "        class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
    "\n",
    "        result: List[Dict[str, torch.Tensor]] = []\n",
    "        detector_losses = {}\n",
    "        loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
    "        detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
    "        boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
    "        num_images = len(boxes)\n",
    "        for i in range(num_images):\n",
    "            result.append(\n",
    "                {\n",
    "                    \"boxes\": boxes[i],\n",
    "                    \"labels\": labels[i],\n",
    "                    \"scores\": scores[i],\n",
    "                }\n",
    "            )\n",
    "        detections = result\n",
    "        detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "        model.rpn.training=False\n",
    "        model.roi_heads.training=False\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "        return losses, detections\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # 배치에서 이미지를 가져오기\n",
    "        # image: torch.Size([3, 1024, 1024])\n",
    "        # target: {\n",
    "        # 'boxes': list of tensor, \n",
    "        # 'labels': list of int, \n",
    "        # 'image_id': int, \n",
    "        # 'area': list of tensor, \n",
    "        # 'iscrowd': list of int (0 or 1)\n",
    "        # }\n",
    "        # image_id: int\n",
    "        images, targets, image_ids = batch\n",
    "\n",
    "        # 모델의 forward pass\n",
    "        loss_dict = self.model(images=images, targets=targets)\n",
    "        \n",
    "        #print(f'{loss_dict}')\n",
    "\n",
    "        # 총 학습 손실 계산\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        #print(f'{losses}')\n",
    "\n",
    "        # 학습 손실 로깅\n",
    "        self.log('train_loss', losses, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # 각 학습 손실 로깅\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step과 동일한 batch\n",
    "\n",
    "        images, targets, image_ids = batch\n",
    "\n",
    "        # 모델의 forward pass\n",
    "        loss_dict, _ = self.eval_forward(images=images, targets=targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())  # 손실 합산\n",
    "        \n",
    "        # 검증 손실 로깅\n",
    "        self.log('val_loss', losses, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # 각 검증 손실 로깅\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"val_\" + k, v.item())\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer와 scheduler 설정\n",
    "        optimizer = torch.optim.SGD(\n",
    "            [\n",
    "                {'params': self.param_dicts['backbone'], 'lr': self.lr_backbone, 'weight_decay': self.weight_decay},  # Classifier에 대한 설정\n",
    "                {'params': self.param_dicts['etc'], 'lr': self.lr, 'weight_decay': self.weight_decay}  # Swin에 대한 설정\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 학습률 스케줄러 설정\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=4, eta_min=1e-6)\n",
    "\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Define train_loader\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=7\n",
    "            )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Define val_loader\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=custom_collate_fn,\n",
    "            num_workers=7\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, max_steps=20000):\n",
    "    # checkpoint 콜백 함수 정의\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints/fasterrcnn\",  # 체크포인트 저장 경로\n",
    "        filename=\"{epoch:02d}-{val_loss:.2f}\",  # 저장될 파일명 포맷\n",
    "        save_top_k=3,  # 상위 몇 개의 모델만 저장할지\n",
    "        monitor=\"val_loss\",  # 검증 손실을 모니터링하여 체크포인트 저장\n",
    "        mode=\"min\",  # 손실이 가장 적을 때 저장 (최소화)\n",
    "        save_weights_only=True  # 전체 모델을 저장 (가중치만 저장하려면 True)\n",
    "    )\n",
    "\n",
    "    # earlystop 콜백 함수 정의\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_loss',  # 모니터링할 지표 (예: val_loss)\n",
    "        patience=3,          # 개선이 없으면 학습을 멈추기까지 대기할 epoch 수\n",
    "        verbose=False,        # 로그 출력 여부\n",
    "        mode='min'           # 지표를 최소화할지('min') 또는 최대화할지('max')\n",
    "    )\n",
    "\n",
    "    # Wandb 로거 생성\n",
    "    wandb_logger = WandbLogger()\n",
    "\n",
    "    trainer = Trainer(max_steps=max_steps, gradient_clip_val=3, callbacks=[checkpoint_callback, early_stop_callback], accelerator='gpu', logger=wandb_logger)\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 데이터셋 불러오기\n",
    "    annotation = '../../../dataset/train.json' # annotation 경로\n",
    "    data_dir = '../../../dataset' # data_dir 경로\n",
    "\n",
    "    # CustomDataset 인스턴스 생성\n",
    "    train_dataset = CustomDataset(annotation, data_dir, transforms=get_train_transform(), train=True)\n",
    "    val_dataset = CustomDataset(annotation, data_dir, transforms=get_valid_transform(), train=False)\n",
    "    \n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(device)\n",
    "    \n",
    "    # load model\n",
    "    model = FasterRCNN(train_dataset, val_dataset, batch_size=16, lr=5e-3, lr_backbone=5e-5, weight_decay=5e-4)\n",
    "    model.to(device)\n",
    "    \n",
    "    # training\n",
    "    train_fn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /data/ephemeral/level2-objectdetection-cv-15/jseo/baseline/faster_rcnn/checkpoints/fasterrcnn exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | FasterRCNN | 41.3 M\n",
      "-------------------------------------\n",
      "41.1 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.3 M    Total params\n",
      "165.381   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  0.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  26%|██▌       | 63/245 [01:01<02:56,  1.03it/s, v_num=zijo, train_loss_step=0.619, val_loss_step=0.561, val_loss_epoch=0.671, train_loss_epoch=0.666] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
